1. clone repo to target destination
2. navigate to airflow directory and execute the following
    docker compose up airflow-init
    docker compose up -d
3. Bring the database up by navigating to sample-project-aa/db and running docker compose up -d
4. To connect to the db use user:airflow password:airflow db:project_data
5. adjust spark to the correct database host
6. navigate to sample-project-aa
7. locate to_text_conversion, url_scraper, yt_mp3_downloader, table-join-and-emotional-analysis, video_filter  directories
8. navigate to each and build the images giving them the names transcribe-many:latest, url-scraper:latest,  mp3-getter:latest, comment-scraper2:latest, spark_job:latest, video_filter:latest respectively
9. To do so use the command docker build -t <image_name> .
10_5. You must create local folders for the temporary files

├── json
│   ├── comments
│   │   ├── -0F7iua-37Y.json (example)
│   └── video
│       ├── 07Zoc5fgoOA.json (example)
├── mp3
│   ├── 07Zoc5fgoOA.mp3 (example)
│
├── text
│   ├── 0iKObc8sCMg.txt (example)
├
└
I would recommend creating a 'data' folder and then create the folders above inside of it.
after doing that you must adjust the 5 Mounts in the project-dag1.py file
To do this easily, search for 'Mount(' and you should find 5 results (dont forget opening parenthesis)
THen ONLY change the source parameter to match the local folder you created
Note: do not mistake json/video with json/comments, they are different.

11. In the cleanup task (towards the end) we are deleting all mp3, json, and text files. Make sure the directories are correct 
11. Ο χρήστης τρέχει την εντολή getent group docker στο terminal, έπειτα πηγαίνει στο αρχείο .env και βάζει το αριθμό που έβαλε στην μεταβλητή DOCKER_GID

12. To connect to airflow now navigate to ip:8080 and use u:airflow p:airflow
13. trigger the dag called project-dag1

config:
you can use the parameters when triggering to adjust nr_workers and debug mode AND search keywords

IMPORTANT:
- ας μη μπλεκουμε διαφορετικα keywords με την ιδια database, αν θες να πειραματιστεις με καινουργια κανε docker compose down -v στη database πρωτα (και παλι up)
- αν θες να εμπλουτισεις ενα run με περισσοτερα αποτελεσματα μπορεισ να το ξανατρεξεις αφοβα. Τα duplicates δε θα επεξεργαστουν οποτε και πολυ πιο γρηγορα θα παει, και τα data μας δε θα χαλασουν
- Το πρωτο run θα παρει ωρες, επειδη αργει πολυ το transcribe. Στα επομενα θα σαι οκ (οπως εξηγησα και πιο πανω)

to stop a run easily
run this
docker rm -f $(docker ps -aq --filter "ancestor=comment-scraper-2:latest" --filter "ancestor=mp3-getter:latest" --filter "ancestor=transcribe-many:latest")

it will remove all containers

to backup database 
docker exec data_warehouse pg_dump -U airflow -d project_data | gzip > backup.sql.gz

