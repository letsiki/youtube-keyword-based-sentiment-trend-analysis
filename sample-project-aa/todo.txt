Part A
Build scraper
Search and extract playlists
use template
https://www.youtube.com/results?search_query={keywords-go-here}&sp=EgIQAw%253D%253D
use urlib.parse.quote('keywords-go-here) to handle spaces
pick the top 10 or something, xcom a list of urls

Part B
# Cleanwich
def split_chunks(data, n_chunks):
    from math import ceil
    avg = ceil(len(data) / n_chunks)
    return [data[i * avg : (i + 1) * avg] for i in range(n_chunks)]
or my version

Part C
get the result of Part B into a variable. 
create a group of tasks. the first one in branch(url)
loop n=4 times and create the groups

Part D
option a:
build the individual pieces of the group and group them
group blueprint:

    branch into leaf A 
                    -> download mp3's pass filepaths via xcom 
                    -> convert to text file pass .txt filepath via xcom
                    -> load into pg table1 (id url, text file contents)
            and leaf B 
                    -> selenium get contents (html) save to html pass html filepath via xcom 
                    -> bs4 parse html files store to json pass json path via xcom 
                    -> load into pg table2 (id url, json file contents as BJSON)

option b:
like option a, but use the mapped task syntax instead of groups (see Extras below)

Part E
Decide what is considered an error and what should happen in case of one


Extras:

A. Checkout this syntax for mapped tasks

assume the following tasks:

@task
def scrape(url):
    html = selenium_scrape(url)
    path = f"/tmp/scrapes/{hash(url)}.html"
    with open(path, "w") as f:
        f.write(html)
    return path

@task
def parse(path):
    with open(path) as f:
        html = f.read()
    comments = extract_comments(html)
    save_to_pg(comments)

you can use this for generating them based on the len of the return of get_urls()

get_urls() >> scrape.expand(url=get_urls()) >> parse.expand(path=scrape.output)

In the dag they will appear as one but with a drop menu

B. In the docker operator, you may use the 'python main.py' as entrypoint, and leave the 
command empty for potential arguments like debug. 
For container inspection you may use docker run -it --entrypoint bash my_image.
With this setup we can configure a manual run that passes the debug variable, that will end
up overriding the cmd and scrape very few urls instead of a regular run (command kwarg in docker operator)